/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

#include <executorch/runtime/core/evalue.h>
#include <executorch/runtime/core/exec_aten/exec_aten.h>
#include <executorch/runtime/core/exec_aten/util/tensor_util.h>
#include <executorch/runtime/core/span.h>
#include <executorch/runtime/kernel/operator_registry.h>
#include <executorch/runtime/platform/profiler.h>
#include "NativeFunctions.h" // Generated Function import headers
// @generated by gen.py from RegisterCodegenUnboxedKernels.cpp

// NOTE [Sharded File]: This file is generated in a sharded fashion to speed up
// incremental rebuilds. See the comment at the top of
// templates/VariableType.cpp for an analogous, in-depth discussion.
//
// Generated by tools/jit/gen_unboxing.py. This file registers all ATen ops into
// JIT op registry instead of c10 dispatcher. JIT op registry only takes boxed
// kernels, so we are calling unboxing functions in UnboxingFunctions.h to cast
// arguments into C++ types (instead of IValue) and delegate to unboxed kernels.
using KernelSpan = ::executorch::runtime::Span<
    const ::executorch::ET_RUNTIME_NAMESPACE::Kernel>;
namespace torch {
namespace executor {
namespace function {
namespace {

static Kernel kernels_to_register[] = {

    Kernel(
        "cortex_m::quantize_per_tensor.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 8, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)8, stack.size());
            EValue& input = *stack[0];
    	EValue& scale = *stack[1];
    	EValue& zero_point = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	double scale_base = scale.to<double>();
    	int64_t zero_point_base = zero_point.to<int64_t>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_quantize_per_tensor.out");
            EXECUTORCH_SCOPE_PROF("native_call_quantize_per_tensor.out");
            cortex_m::native::quantize_per_tensor_out(context, input_base, scale_base, zero_point_base, quant_min_base, quant_max_base, dtype_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);



        }
    ),

    Kernel(
        "cortex_m::dequantize_per_tensor.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 8, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)8, stack.size());
            EValue& input = *stack[0];
    	EValue& scale = *stack[1];
    	EValue& zero_point = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	double scale_base = scale.to<double>();
    	int64_t zero_point_base = zero_point.to<int64_t>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_dequantize_per_tensor.out");
            EXECUTORCH_SCOPE_PROF("native_call_dequantize_per_tensor.out");
            cortex_m::native::dequantize_per_tensor_out(context, input_base, scale_base, zero_point_base, quant_min_base, quant_max_base, dtype_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);



        }
    ),

    Kernel(
        "cortex_m::quantized_add",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 12, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)12, stack.size());
            EValue& self = *stack[0];
    	EValue& self_zero_point = *stack[1];
    	EValue& self_multiplier = *stack[2];
    	EValue& self_shift = *stack[3];
    	EValue& other = *stack[4];
    	EValue& other_zero_point = *stack[5];
    	EValue& other_multiplier = *stack[6];
    	EValue& other_shift = *stack[7];
    	EValue& output_zero_point = *stack[8];
    	EValue& output_multiplier = *stack[9];
    	EValue& output_shift = *stack[10];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & self_zero_point_base = self_zero_point.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & self_multiplier_base = self_multiplier.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & self_shift_base = self_shift.to<torch::executor::Scalar>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_zero_point_base = other_zero_point.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & other_multiplier_base = other_multiplier.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & other_shift_base = other_shift.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & output_zero_point_base = output_zero_point.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & output_multiplier_base = output_multiplier.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & output_shift_base = output_shift.to<torch::executor::Scalar>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_quantized_add");
            EXECUTORCH_SCOPE_PROF("native_call_quantized_add");
            torch::executor::Tensor result_ = cortex_m::native::quantized_add(context, self_base, self_zero_point_base, self_multiplier_base, self_shift_base, other_base, other_zero_point_base, other_multiplier_base, other_shift_base, output_zero_point_base, output_multiplier_base, output_shift_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[11]);

            *stack[11] = EValue(result_);

        }
    ),

    Kernel(
        "cortex_m::quantized_add.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 13, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)13, stack.size());
            EValue& self = *stack[0];
    	EValue& self_zero_point = *stack[1];
    	EValue& self_multiplier = *stack[2];
    	EValue& self_shift = *stack[3];
    	EValue& other = *stack[4];
    	EValue& other_zero_point = *stack[5];
    	EValue& other_multiplier = *stack[6];
    	EValue& other_shift = *stack[7];
    	EValue& output_zero_point = *stack[8];
    	EValue& output_multiplier = *stack[9];
    	EValue& output_shift = *stack[10];
    	EValue& out = *stack[11];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & self_zero_point_base = self_zero_point.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & self_multiplier_base = self_multiplier.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & self_shift_base = self_shift.to<torch::executor::Scalar>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & other_zero_point_base = other_zero_point.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & other_multiplier_base = other_multiplier.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & other_shift_base = other_shift.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & output_zero_point_base = output_zero_point.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & output_multiplier_base = output_multiplier.to<torch::executor::Scalar>();
    	const torch::executor::Scalar & output_shift_base = output_shift.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_quantized_add.out");
            EXECUTORCH_SCOPE_PROF("native_call_quantized_add.out");
            cortex_m::native::quantized_add_out(context, self_base, self_zero_point_base, self_multiplier_base, self_shift_base, other_base, other_zero_point_base, other_multiplier_base, other_shift_base, output_zero_point_base, output_multiplier_base, output_shift_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[11]);



        }
    ), // Generated kernels
};

// Explicitly convert to Span, so that the API can take an empty C array of
// Kernels.
static KernelSpan kernel_span(
    kernels_to_register,
    kernels_to_register + sizeof(kernels_to_register) / sizeof(Kernel));

// Return value not used. Keep the static variable assignment to register
// kernels in static initialization time.
static auto success_with_kernel_reg = register_kernels(kernel_span);
} // namespace
} // namespace function
} // namespace executor
} // namespace torch
