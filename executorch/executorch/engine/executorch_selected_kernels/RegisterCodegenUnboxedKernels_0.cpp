/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

#include <executorch/runtime/core/evalue.h>
#include <executorch/runtime/core/exec_aten/exec_aten.h>
#include <executorch/runtime/core/exec_aten/util/tensor_util.h>
#include <executorch/runtime/core/span.h>
#include <executorch/runtime/kernel/operator_registry.h>
#include <executorch/runtime/platform/profiler.h>
#include "NativeFunctions.h" // Generated Function import headers
// @generated by gen.py from RegisterCodegenUnboxedKernels.cpp

// NOTE [Sharded File]: This file is generated in a sharded fashion to speed up
// incremental rebuilds. See the comment at the top of
// templates/VariableType.cpp for an analogous, in-depth discussion.
//
// Generated by tools/jit/gen_unboxing.py. This file registers all ATen ops into
// JIT op registry instead of c10 dispatcher. JIT op registry only takes boxed
// kernels, so we are calling unboxing functions in UnboxingFunctions.h to cast
// arguments into C++ types (instead of IValue) and delegate to unboxed kernels.
using KernelSpan = ::executorch::runtime::Span<
    const ::executorch::ET_RUNTIME_NAMESPACE::Kernel>;
namespace torch {
namespace executor {
namespace function {
namespace {

static Kernel kernels_to_register[] = {

    Kernel(
        "aten::add.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 5, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)5, stack.size());
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& alpha = *stack[2];
    	EValue& out = *stack[3];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	const torch::executor::Scalar & alpha_base = alpha.to<torch::executor::Scalar>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_add.out");
            EXECUTORCH_SCOPE_PROF("native_call_add.out");
            torch::executor::native::add_out(context, self_base, other_base, alpha_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);



        }
    ),

    Kernel(
        "aten::mul.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 4, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)4, stack.size());
            EValue& self = *stack[0];
    	EValue& other = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & other_base = other.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_mul.out");
            EXECUTORCH_SCOPE_PROF("native_call_mul.out");
            torch::executor::native::mul_out(context, self_base, other_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);



        }
    ),

    Kernel(
        "aten::permute_copy.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 4, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)4, stack.size());
            EValue& self = *stack[0];
    	EValue& dims = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();

    	    auto dims_list_out = dims.toIntList();

    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_permute_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_permute_copy.out");
            torch::executor::native::permute_copy_out(context, self_base, dims_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);



        }
    ),

    Kernel(
        "aten::relu.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 3, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)3, stack.size());
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_relu.out");
            EXECUTORCH_SCOPE_PROF("native_call_relu.out");
            torch::executor::native::relu_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);



        }
    ),

    Kernel(
        "aten::sigmoid.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 3, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)3, stack.size());
            EValue& self = *stack[0];
    	EValue& out = *stack[1];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_sigmoid.out");
            EXECUTORCH_SCOPE_PROF("native_call_sigmoid.out");
            torch::executor::native::sigmoid_out(context, self_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[1]);



        }
    ),

    Kernel(
        "aten::view_copy.out",
        [](torch::executor::KernelRuntimeContext & context, Span<EValue*> stack) {
            ET_KERNEL_CHECK_MSG(context, stack.size() == 4, InvalidProgram, /*void*/, "Expected %" ET_PRIsize_t "args received %" ET_PRIsize_t, (size_t)4, stack.size());
            EValue& self = *stack[0];
    	EValue& size = *stack[1];
    	EValue& out = *stack[2];
    	const torch::executor::Tensor & self_base = self.to<torch::executor::Tensor>();

    	    auto size_list_out = size.toIntList();

    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();


            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_view_copy.out");
            EXECUTORCH_SCOPE_PROF("native_call_view_copy.out");
            torch::executor::native::view_copy_out(context, self_base, size_list_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);



        }
    ), // Generated kernels
};

// Explicitly convert to Span, so that the API can take an empty C array of
// Kernels.
static KernelSpan kernel_span(
    kernels_to_register,
    kernels_to_register + sizeof(kernels_to_register) / sizeof(Kernel));

// Return value not used. Keep the static variable assignment to register
// kernels in static initialization time.
static auto success_with_kernel_reg = register_kernels(kernel_span);
} // namespace
} // namespace function
} // namespace executor
} // namespace torch
