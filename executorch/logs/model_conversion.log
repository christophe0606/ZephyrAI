Skipping import of cpp extensions due to incompatible torch version 2.9.0+cpu for torchao version 0.14.0+cpu             Please see https://github.com/pytorch/ao/issues/2919 for more info

[ Before Graph Optimisation ]
0     Const                aten_add_tensor_output_zp     
1     Const                aten_add_tensor_input_zp      
2     Const                aten_add_tensor_shifts        
3     Const                aten_add_tensor_multipliers   
4     Const                layer-5_output_zp             
5     Const                layer-5_input_zp              
6     Const                layer-5_shifts                
7     Const                layer-5_multipliers           
8     Const                aten_mul_tensor_output_zp     
9     Const                aten_mul_tensor_input_zp      
10    Const                aten_mul_tensor_shifts        
11    Const                aten_mul_tensor_multipliers   
12    Const                aten_mul_tensor_shift         
13    Const                layer-2_output_zp             
14    Const                layer-2_input_zp              
15    Const                layer-2_shifts                
16    Const                layer-2_multipliers           
17    Const                b__frozen_param0              
18    Rescale              layer-2                       
19    Const                layer-1_output_zp             
20    Const                layer-1_input_zp              
21    Const                layer-1_shifts                
22    Const                layer-1_multipliers           
23    Transpose            tosa_transpose_default_1      
24    Rescale              layer-1                       
25    Mul                  layer-3                       
26    Rescale              aten_mul_tensor               
27    Rescale              layer-5                       
28    Const                layer-4_output_zp             
29    Const                layer-4_input_zp              
30    Const                layer-4_shifts                
31    Const                layer-4_multipliers           
32    Transpose            tosa_transpose_default        
33    Rescale              layer-4                       
34    Add                  layer-6                       
35    Rescale              aten_add_tensor               
36    Transpose            tosa_transpose_default_2      


[ After Graph Optimization ]
0     Const                aten_add_tensor_output_zp     
1     Const                aten_add_tensor_input_zp      
2     Const                aten_add_tensor_shifts        
3     Const                aten_add_tensor_multipliers   
4     Const                layer-5_output_zp             
5     Const                layer-5_input_zp              
6     Const                layer-5_shifts                
7     Const                layer-5_multipliers           
8     Const                aten_mul_tensor_output_zp     
9     Const                aten_mul_tensor_input_zp      
10    Const                aten_mul_tensor_shifts        
11    Const                aten_mul_tensor_multipliers   
12    Const                aten_mul_tensor_shift         
13    Const                layer-2_output_zp             
14    Const                layer-2_input_zp              
15    Const                layer-2_shifts                
16    Const                layer-2_multipliers           
17    Const                b__frozen_param0              
18    Rescale              layer-2                       
19    Const                layer-1_output_zp             
20    Const                layer-1_input_zp              
21    Const                layer-1_shifts                
22    Const                layer-1_multipliers           
23    Transpose            tosa_transpose_default_1      
24    Rescale              layer-1                       
25    Mul                  layer-3                       
26    Rescale              aten_mul_tensor               
27    Rescale              layer-5                       
28    Const                layer-4_output_zp             
29    Const                layer-4_input_zp              
30    Const                layer-4_shifts                
31    Const                layer-4_multipliers           
32    Transpose            tosa_transpose_default        
33    Rescale              layer-4                       
34    Add                  layer-6                       
35    Rescale              aten_add_tensor               
36    Transpose            tosa_transpose_default_2      


[ Graph With Tensor Quantization ]
0 Const aten_add_tensor_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_output_zp
1 Const aten_add_tensor_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_input_zp
2 Const aten_add_tensor_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_shifts
3 Const aten_add_tensor_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_multipliers
4 Const layer-5_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_output_zp
5 Const layer-5_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_input_zp
6 Const layer-5_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_shifts
7 Const layer-5_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_multipliers
8 Const aten_mul_tensor_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_output_zp
9 Const aten_mul_tensor_input_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_input_zp
10 Const aten_mul_tensor_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_shifts
11 Const aten_mul_tensor_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_multipliers
12 Const aten_mul_tensor_shift
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_shift
13 Const layer-2_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_output_zp
14 Const layer-2_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_input_zp
15 Const layer-2_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_shifts
16 Const layer-2_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_multipliers
17 Const b__frozen_param0
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 b__frozen_param0
18 Rescale layer-2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 b__frozen_param0
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_shifts
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_input_zp
    Input 04 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2
19 Const layer-1_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_output_zp
20 Const layer-1_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_input_zp
21 Const layer-1_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_shifts
22 Const layer-1_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_multipliers
23 Transpose tosa_transpose_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default_1
24 Rescale layer-1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default_1
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_shifts
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_input_zp
    Input 04 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1
25 Mul layer-3
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-1
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-2
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_shift
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3
26 Rescale aten_mul_tensor
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-3
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor
27 Rescale layer-5
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_shifts
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_input_zp
    Input 04 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
28 Const layer-4_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_output_zp
29 Const layer-4_input_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_input_zp
30 Const layer-4_shifts
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_shifts
31 Const layer-4_multipliers
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_multipliers
32 Transpose tosa_transpose_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
33 Rescale layer-4
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_shifts
    Input 03 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_input_zp
    Input 04 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4_output_zp
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
34 Add layer-6
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
35 Rescale aten_add_tensor
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_multipliers
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_shifts
    Input 03 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_input_zp
    Input 04 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor_output_zp
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor
36 Transpose tosa_transpose_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default_2


[ Before Graph Optimisation ]
0     Const                aten_add_tensor_output_zp     
1     Const                aten_add_tensor_input_zp      
2     Const                aten_add_tensor_shifts        
3     Const                aten_add_tensor_multipliers   
4     Const                layer-5_output_zp             
5     Const                layer-5_input_zp              
6     Const                layer-5_shifts                
7     Const                layer-5_multipliers           
8     Const                aten_mul_tensor_output_zp     
9     Const                aten_mul_tensor_input_zp      
10    Const                aten_mul_tensor_shifts        
11    Const                aten_mul_tensor_multipliers   
12    Const                aten_mul_tensor_shift         
13    Const                layer-2_output_zp             
14    Const                layer-2_input_zp              
15    Const                layer-2_shifts                
16    Const                layer-2_multipliers           
17    Const                b__frozen_param0              
18    Rescale              layer-2                       
19    Const                layer-1_output_zp             
20    Const                layer-1_input_zp              
21    Const                layer-1_shifts                
22    Const                layer-1_multipliers           
23    Transpose            tosa_transpose_default_1      
24    Rescale              layer-1                       
25    Mul                  layer-3                       
26    Rescale              aten_mul_tensor               
27    Rescale              layer-5                       
28    Const                layer-4_output_zp             
29    Const                layer-4_input_zp              
30    Const                layer-4_shifts                
31    Const                layer-4_multipliers           
32    Transpose            tosa_transpose_default        
33    Rescale              layer-4                       
34    Add                  layer-6                       
35    Rescale              aten_add_tensor               
36    Transpose            tosa_transpose_default_2      


[ After Graph Optimization ]
0     Transpose            tosa_transpose_default_1      
1     Mul                  aten_mul_tensor               
2     Add                  aten_mul_tensor               
3     Mul                  layer-5                       
4     Transpose            tosa_transpose_default        
5     Add                  tosa_transpose_default        
6     Mul                  layer-4                       
7     Add                  layer-6                       
8     Mul                  aten_add_tensor               
9     Transpose            tosa_transpose_default_2      


[ Graph With Tensor Quantization ]
0 Transpose tosa_transpose_default_1
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default_1
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default_1
1 Mul aten_mul_tensor
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default_1
    Input 01 Int8 scale: [(scale:1, shift:0)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 const_values
    Input 02 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 const_values
    Output 00 Int8 scale: [(scale:1077952640, shift:38)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor
2 Add aten_mul_tensor
    Input 00 Int8 scale: [], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor
    Input 01 Int8 scale: [], zero_point: [], quantMin: [], quantMax: [], dimension: 0 const_zero
    Output 00 Int32 scale: [], zero_point: [], quantMin: [-9223372036854775808], quantMax: [9223372036854775807], dimension: 0 aten_mul_tensor
3 Mul layer-5
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 multipliers_0_0
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_mul_tensor
    Output 00 Int32 scale: [(scale:1, shift:11)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
4 Transpose tosa_transpose_default
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 quantized_decomposed_quantize_per_tensor_default
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
5 Add tosa_transpose_default
    Input 00 Int8 scale: [], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
    Input 01 Int8 scale: [], zero_point: [], quantMin: [], quantMax: [], dimension: 0 const_zero
    Output 00 Int32 scale: [], zero_point: [], quantMin: [-9223372036854775808], quantMax: [9223372036854775807], dimension: 0 tosa_transpose_default
6 Mul layer-4
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 multipliers_0_0
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default
    Output 00 Int32 scale: [(scale:1, shift:12)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
7 Add layer-6
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-4
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-5
    Output 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
8 Mul aten_add_tensor
    Input 00 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 multipliers_0_0
    Input 01 Int32 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 layer-6
    Output 00 Int8 scale: [(scale:1, shift:50)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor
9 Transpose tosa_transpose_default_2
    Input 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 aten_add_tensor
    Output 00 Int8 scale: [(scale:1, shift:0)], zero_point: [0], quantMin: [], quantMax: [], dimension: 0 tosa_transpose_default_2

Schedule: 'graph_MAX_BUFFERED'
	0: Operation Transpose  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 0
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=32
		Memory Used: 48 bytes
	1: Operation Transpose  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 2
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=32
		Memory Used: 48 bytes
	2: Operation Mul  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 4
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = [1, 1, 1, 1]
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=32
		Memory Used: 48 bytes
	3: Operation Add  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 6
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = [1, 1, 1, 1]
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=34
		Memory Used: 96 bytes
	4: Operation Mul  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 8
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = [1, 1, 1, 1]
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=33
		Memory Used: 80 bytes
	5: Operation Transpose  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 10
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=32
		Memory Used: 96 bytes
	6: Operation Transpose  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 12
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=32
		Memory Used: 96 bytes
	7: Operation Add  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 14
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = [1, 1, 1, 1]
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=34
		Memory Used: 144 bytes
	8: Operation Mul  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 16
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = [1, 1, 1, 1]
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=33
		Memory Used: 128 bytes
	9: Operation Add  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 18
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = [1, 1, 1, 1]
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=21
		Memory Used: 128 bytes
	10: Operation Mul  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 20
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = [1, 1, 1, 1]
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=2
		Memory Used: 80 bytes
	11: Operation Transpose  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 22
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=32
		Memory Used: 32 bytes
	12: Operation Transpose  - OFM 1, 1, 1, 1
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 24
		Operator Config = OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
		IFM Stripe   = [1, 1, 1, 1]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 1]
		Assigned Cascade = 0
		sub-operations: -
		Estimated Perf: Macs=0 Cycles=32
		Memory Used: 32 bytes
	Cascades:
################################################################################
Allocation, memory Sram, usage mask: FeatureMap|Staging
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         0 -          1:       0x10 -       0x20:          16:           48 : quantized_decomposed_quantize_per_tensor_default_1
         0 -          3:        0x0 -       0x10:          16:           48 : ? (uid 147)
         0 -         11:       0x40 -       0x50:          16:           96 : quantized_decomposed_quantize_per_tensor_default
         2 -          5:       0x10 -       0x20:          16:           48 : tosa_transpose_default_1
         4 -          7:       0x50 -       0x60:          16:           96 : aten_mul_tensor
         6 -         19:        0x0 -       0x40:          64:          144 : layer-5
         6 -         19:        0x0 -       0x40:          64:          144 : aten_mul_tensor
        10 -         13:       0x50 -       0x60:          16:           96 : ? (uid 166)
        12 -         15:       0x80 -       0x90:          16:          144 : tosa_transpose_default
        14 -         21:       0x40 -       0x80:          64:          144 : layer-6
        14 -         21:       0x40 -       0x80:          64:          144 : layer-4
        14 -         21:       0x40 -       0x80:          64:          144 : tosa_transpose_default
        20 -         23:        0x0 -       0x10:          16:           80 : aten_add_tensor
        22 -         25:       0x10 -       0x20:          16:           32 : ? (uid 185)
        24 -         27:        0x0 -       0x10:          16:           32 : tosa_transpose_default_2
Allocation Peak Tensor Size: 144 bytes == 0.140625 KiB
################################################################################
Tensor Allocation for read-only NPU tensors:
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         4 -          5:        0x0 -       0x10:          16:           16 : const_values
         6 -         15:       0x10 -       0x20:          16:           32 : const_zero
         6 -         15:       0x10 -       0x20:          16:           32 : const_zero
         8 -         17:       0x20 -       0x30:          16:           32 : multipliers_0_0
         8 -         17:       0x20 -       0x30:          16:           32 : multipliers_0_0
        20 -         21:       0x30 -       0x40:          16:           16 : multipliers_0_0
Allocation Peak Tensor Size: 64 bytes == 0.0625 KiB
High level NPU operations:
0 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: quantized_decomposed_quantize_per_tensor_default_1, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x10
  OFM: ? (uid 147), [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x0
1 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: ? (uid 147), [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x0
  OFM: tosa_transpose_default_1, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x10
2 Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: tosa_transpose_default_1, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x10
  IFM2: const_values, [1, 1, 1, 1], format: 1, OffChipFlash:ReadOnly, address: 0x0
  OFM: aten_mul_tensor, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x50
3 Add , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: aten_mul_tensor, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x50
  IFM2: const_zero, [1, 1, 1, 1], format: 1, OffChipFlash:ReadOnly, address: 0x10
  OFM: aten_mul_tensor, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x0
4 Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: aten_mul_tensor, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x0
  IFM2: multipliers_0_0, [1, 1, 1, 1], format: 1, OffChipFlash:ReadOnly, address: 0x20
  OFM: layer-5, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x0
5 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: quantized_decomposed_quantize_per_tensor_default, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x40
  OFM: ? (uid 166), [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x50
6 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: ? (uid 166), [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x50
  OFM: tosa_transpose_default, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x80
7 Add , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: tosa_transpose_default, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x80
  IFM2: const_zero, [1, 1, 1, 1], format: 1, OffChipFlash:ReadOnly, address: 0x10
  OFM: tosa_transpose_default, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x40
8 Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: tosa_transpose_default, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x40
  IFM2: multipliers_0_0, [1, 1, 1, 1], format: 1, OffChipFlash:ReadOnly, address: 0x20
  OFM: layer-4, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x40
9 Add , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: layer-4, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x40
  IFM2: layer-5, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x0
  OFM: layer-6, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x40
10 Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: layer-6, [1, 1, 1, 1], format: 2, Sram:FeatureMap|Staging, address: 0x40
  IFM2: multipliers_0_0, [1, 1, 1, 1], format: 1, OffChipFlash:ReadOnly, address: 0x30
  OFM: aten_add_tensor, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x0
11 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: aten_add_tensor, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x0
  OFM: ? (uid 185), [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x10
12 Transpose , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
  IFM: ? (uid 185), [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x10
  OFM: tosa_transpose_default_2, [1, 1, 1, 1], format: 1, Sram:FeatureMap|Staging, address: 0x0
High level command stream:
0 Transpose OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1]
1 Transpose OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1]
2 Mul OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1], IFM2 [0, 0, 0, 0 - 1, 1, 1, 1]
3 Add OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1], IFM2 [0, 0, 0, 0 - 1, 1, 1, 1]
4 Mul OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1], IFM2 [0, 0, 0, 0 - 1, 1, 1, 1]
5 Transpose OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1]
6 Transpose OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1]
7 Add OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1], IFM2 [0, 0, 0, 0 - 1, 1, 1, 1]
8 Mul OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1], IFM2 [0, 0, 0, 0 - 1, 1, 1, 1]
9 Add OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1], IFM2 [0, 0, 0, 0 - 1, 1, 1, 1]
10 Mul OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1], IFM2 [0, 0, 0, 0 - 1, 1, 1, 1]
11 Transpose OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1]
12 Transpose OFM area [0, 0, 0, 0 - 1, 1, 1, 1], IFM [0, 0, 0, 0 - 1, 1, 1, 1]
RCS: Emitting no-op transpose as a memory copy
RCS: Emitting no-op transpose as a memory copy
RCS: Emitting no-op transpose as a memory copy
RCS: Emitting no-op transpose as a memory copy
RCS: Emitting no-op transpose as a memory copy
RCS: Emitting no-op transpose as a memory copy
Register command stream: 247 words
  Offset: Payload Param Code - Command                        Param, Fields
// DMA src: Sram:FeatureMap|Staging, address: 0x10, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 1
0x000000:          0001 0130 - NPU_SET_DMA0_SRC_REGION            1, region = 1, region_mode = DMA_REGION_MODE_EXTERNAL, stride_mode = DMA_STRIDE_MODE_D1
0x000004: 00000010 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x10
0x00000c:          0001 0131 - NPU_SET_DMA0_DST_REGION            1, region = 1, region_mode = DMA_REGION_MODE_EXTERNAL, stride_mode = DMA_STRIDE_MODE_D1
0x000010: 00000000 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x0
0x000018: 00000001 0000 4032 - NPU_SET_DMA0_LEN                   0, addr = 0x1
0x000020:          0000 0010 - NPU_OP_DMA_START                   0
// DMA src: Sram:FeatureMap|Staging, address: 0x0, dest: Sram:FeatureMap|Staging, address: 0x10, sizes: (N/A), length: 1
0x000024: 00000000 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x0
0x00002c: 00000010 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x10
0x000034:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000038:          0000 0010 - NPU_OP_DMA_START                   0
// Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
0x00003c: 40404080 0026 4024 - NPU_SET_OFM_SCALE                 38, shift = 38, scale = 1077952640
0x000044:          0001 010f - NPU_SET_IFM_REGION                 1, region = 1
0x000048: 00000010 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x10
0x000050: 00000000 0000 4001 - NPU_SET_IFM_BASE1                  0, addr = 0x0
0x000058: 00000000 0000 4002 - NPU_SET_IFM_BASE2                  0, addr = 0x0
0x000060: 00000000 0000 4003 - NPU_SET_IFM_BASE3                  0, addr = 0x0
0x000068:          0000 010b - NPU_SET_IFM_HEIGHT0_M1             0, height_m1 = 0
0x00006c:          0000 010c - NPU_SET_IFM_HEIGHT1_M1             0, height_m1 = 0
0x000070:          0000 010a - NPU_SET_IFM_WIDTH0_M1              0, width_m1 = 0
0x000074:          0000 0104 - NPU_SET_IFM_DEPTH_M1               0, depth_m1 = 0
0x000078: 00000001 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1
0x000080: 00000001 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x1
0x000088: 00000001 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1
0x000090:          ff80 0109 - NPU_SET_IFM_ZERO_POINT         65408, zero_point = 65408
0x000094:          8001 0105 - NPU_SET_IFM_PRECISION          32769, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = IFM_SCALE_MODE_OPA_OPB_16, round_mode = ROUND_MODE_NATURAL
0x000098:          0000 0107 - NPU_SET_IFM_UPSCALE                0, mode = IFM_UPSCALE_MODE_NONE
0x00009c:          0001 011f - NPU_SET_OFM_REGION                 1, region = 1
0x0000a0: 00000050 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x50
0x0000a8: 00000000 0000 4011 - NPU_SET_OFM_BASE1                  0, addr = 0x0
0x0000b0: 00000000 0000 4012 - NPU_SET_OFM_BASE2                  0, addr = 0x0
0x0000b8: 00000000 0000 4013 - NPU_SET_OFM_BASE3                  0, addr = 0x0
0x0000c0:          0000 0112 - NPU_SET_OFM_HEIGHT_M1              0, height_m1 = 0
0x0000c4:          0000 0111 - NPU_SET_OFM_WIDTH_M1               0, width_m1 = 0
0x0000c8:          0000 011b - NPU_SET_OFM_HEIGHT0_M1             0, height_m1 = 0
0x0000cc:          0000 011c - NPU_SET_OFM_HEIGHT1_M1             0, height_m1 = 0
0x0000d0:          0000 011a - NPU_SET_OFM_WIDTH0_M1              0, width_m1 = 0
0x0000d4:          0000 0113 - NPU_SET_OFM_DEPTH_M1               0, depth_m1 = 0
0x0000d8: 00000010 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x10
0x0000e0: 00000010 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x10
0x0000e8: 00000010 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x10
0x0000f0:          ff80 0118 - NPU_SET_OFM_ZERO_POINT         65408, zero_point = 65408
0x0000f4:          8141 0114 - NPU_SET_OFM_PRECISION          33089, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, round_mode = ROUND_MODE_NATURAL
0x0000f8:          0000 0125 - NPU_SET_ACTIVATION                 0, activation_function = ACTIVATION_FUNCTION_RELU, activation_clip_range = ACTIVATION_CLIP_RANGE_OFM_PRECISION
0x0000fc:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000100:          007f 0127 - NPU_SET_ACTIVATION_MAX           127, clip_boundary = 127
0x000104:          007f 0181 - NPU_SET_IFM2_SCALAR              127, scalar = 127
0x000108:          ff80 0189 - NPU_SET_IFM2_ZERO_POINT        65408, zero_point = 65408
0x00010c:          0001 0185 - NPU_SET_IFM2_PRECISION             1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC
0x000110:          0080 0180 - NPU_SET_IFM2_BROADCAST           128, broadcast_h = BROADCAST_MODE_DISABLE, broadcast_w = BROADCAST_MODE_DISABLE, broadcast_c = BROADCAST_MODE_DISABLE, operand_order = IFM2_OPERAND_ORDER_ORDER_B, broadcast_constant = BROADCAST_MODE_ENABLE
0x000114:          0000 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          0, height_m1 = 0
0x000118:          0001 0115 - NPU_SET_OFM_BLK_WIDTH_M1           1, width_m1 = 1
0x00011c:          0007 0117 - NPU_SET_OFM_BLK_DEPTH_M1           7, depth_m1 = 7
0x000120:          0016 010d - NPU_SET_IFM_IB_END                22, ib_end = 22
0x000124:          0016 012d - NPU_SET_AB_START                  22, ab_start = 22
0x000128:          0006 018d - NPU_SET_IFM2_IB_START              6, ib_start = 6
0x00012c:          0000 0124 - NPU_SET_ACC_FORMAT                 0, acc_format = ACC_FORMAT_I32
0x000130:          0000 012f - NPU_SET_BLOCKDEP                   0, blockdep = 0
0x000134:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x000138:          0000 0006 - NPU_OP_ELEMENTWISE                 0, elementwise_mode = ELEMENTWISE_MODE_MUL
// Add , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
0x00013c: 00000001 0000 4025 - NPU_SET_OPA_SCALE                  0, shift = 0, scale = 1
0x000144: 00000001 0000 4026 - NPU_SET_OPB_SCALE                  0, scale = 1
0x00014c: 00000001 0000 4024 - NPU_SET_OFM_SCALE                  0, shift = 0, scale = 1
0x000154: 00000050 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x50
0x00015c: 00000010 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x10
0x000164: 00000010 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x10
0x00016c: 00000010 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x10
0x000174:          0041 0105 - NPU_SET_IFM_PRECISION             65, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = IFM_SCALE_MODE_OPA_OPB_16, round_mode = ROUND_MODE_DBL
0x000178: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000180: 00000040 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x40
0x000188: 00000040 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x40
0x000190: 00000040 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x40
0x000198:          0000 0118 - NPU_SET_OFM_ZERO_POINT             0, zero_point = 0
0x00019c:          0145 0114 - NPU_SET_OFM_PRECISION            325, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, round_mode = ROUND_MODE_DBL
0x0001a0:          8000 0126 - NPU_SET_ACTIVATION_MIN         32768, clip_boundary = 32768
0x0001a4:          7fff 0127 - NPU_SET_ACTIVATION_MAX         32767, clip_boundary = 32767
0x0001a8:          0000 0181 - NPU_SET_IFM2_SCALAR                0, scalar = 0
0x0001ac:          0000 0189 - NPU_SET_IFM2_ZERO_POINT            0, zero_point = 0
0x0001b0:          0001 0006 - NPU_OP_ELEMENTWISE                 1, elementwise_mode = ELEMENTWISE_MODE_ADD
// Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
0x0001b4: 00000001 000b 4024 - NPU_SET_OFM_SCALE                 11, shift = 11, scale = 1
0x0001bc: 00000000 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x0
0x0001c4: 00000040 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x40
0x0001cc: 00000040 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x40
0x0001d4: 00000040 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x40
0x0001dc:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0001e0:          0049 0105 - NPU_SET_IFM_PRECISION             73, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = IFM_SCALE_MODE_OPA_OPB_16, round_mode = ROUND_MODE_DBL
0x0001e4:          8145 0114 - NPU_SET_OFM_PRECISION          33093, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, round_mode = ROUND_MODE_NATURAL
0x0001e8:          0000 018f - NPU_SET_IFM2_REGION                0, region = 0
0x0001ec: 00000020 0000 4080 - NPU_SET_IFM2_BASE0                 0, addr = 0x20
0x0001f4: 00000000 0000 4081 - NPU_SET_IFM2_BASE1                 0, addr = 0x0
0x0001fc: 00000000 0000 4082 - NPU_SET_IFM2_BASE2                 0, addr = 0x0
0x000204: 00000000 0000 4083 - NPU_SET_IFM2_BASE3                 0, addr = 0x0
0x00020c:          0000 018b - NPU_SET_IFM2_HEIGHT0_M1            0, height_m1 = 0
0x000210:          0000 018c - NPU_SET_IFM2_HEIGHT1_M1            0, height_m1 = 0
0x000214:          0000 018a - NPU_SET_IFM2_WIDTH0_M1             0, width_m1 = 0
0x000218: 00000004 0000 4085 - NPU_SET_IFM2_STRIDE_Y              0, addr = 0x4
0x000220: 00000004 0000 4084 - NPU_SET_IFM2_STRIDE_X              0, addr = 0x4
0x000228: 00000004 0000 4086 - NPU_SET_IFM2_STRIDE_C              0, addr = 0x4
0x000230:          0009 0185 - NPU_SET_IFM2_PRECISION             9, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHWC
0x000234:          0000 0180 - NPU_SET_IFM2_BROADCAST             0, broadcast_h = BROADCAST_MODE_DISABLE, broadcast_w = BROADCAST_MODE_DISABLE, broadcast_c = BROADCAST_MODE_DISABLE, operand_order = IFM2_OPERAND_ORDER_ORDER_B, broadcast_constant = BROADCAST_MODE_DISABLE
0x000238:          000a 018d - NPU_SET_IFM2_IB_START             10, ib_start = 10
0x00023c:          0000 0006 - NPU_OP_ELEMENTWISE                 0, elementwise_mode = ELEMENTWISE_MODE_MUL
// DMA src: Sram:FeatureMap|Staging, address: 0x40, dest: Sram:FeatureMap|Staging, address: 0x50, sizes: (N/A), length: 1
0x000240: 00000040 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x40
0x000248: 00000050 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x50
0x000250:          0001 0012 - NPU_OP_KERNEL_WAIT                 1, n = 1
0x000254:          0000 0010 - NPU_OP_DMA_START                   0
// DMA src: Sram:FeatureMap|Staging, address: 0x50, dest: Sram:FeatureMap|Staging, address: 0x80, sizes: (N/A), length: 1
0x000258: 00000050 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x50
0x000260: 00000080 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x80
0x000268:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x00026c:          0000 0010 - NPU_OP_DMA_START                   0
// Add , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
0x000270: 00000001 0000 4024 - NPU_SET_OFM_SCALE                  0, shift = 0, scale = 1
0x000278: 00000080 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x80
0x000280: 00000001 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x1
0x000288: 00000001 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x1
0x000290: 00000001 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1
0x000298:          ff80 0109 - NPU_SET_IFM_ZERO_POINT         65408, zero_point = 65408
0x00029c:          0001 0105 - NPU_SET_IFM_PRECISION              1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = IFM_SCALE_MODE_OPA_OPB_16, round_mode = ROUND_MODE_DBL
0x0002a0: 00000040 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x40
0x0002a8:          0145 0114 - NPU_SET_OFM_PRECISION            325, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, round_mode = ROUND_MODE_DBL
0x0002ac:          0001 0185 - NPU_SET_IFM2_PRECISION             1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC
0x0002b0:          0080 0180 - NPU_SET_IFM2_BROADCAST           128, broadcast_h = BROADCAST_MODE_DISABLE, broadcast_w = BROADCAST_MODE_DISABLE, broadcast_c = BROADCAST_MODE_DISABLE, operand_order = IFM2_OPERAND_ORDER_ORDER_B, broadcast_constant = BROADCAST_MODE_ENABLE
0x0002b4:          0006 018d - NPU_SET_IFM2_IB_START              6, ib_start = 6
0x0002b8:          0003 012f - NPU_SET_BLOCKDEP                   3, blockdep = 3
0x0002bc:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0002c0:          0001 0006 - NPU_OP_ELEMENTWISE                 1, elementwise_mode = ELEMENTWISE_MODE_ADD
// Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
0x0002c4: 00000001 000c 4024 - NPU_SET_OFM_SCALE                 12, shift = 12, scale = 1
0x0002cc: 00000040 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x40
0x0002d4: 00000040 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0x40
0x0002dc: 00000040 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0x40
0x0002e4: 00000040 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x40
0x0002ec:          0000 0109 - NPU_SET_IFM_ZERO_POINT             0, zero_point = 0
0x0002f0:          0049 0105 - NPU_SET_IFM_PRECISION             73, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = IFM_SCALE_MODE_OPA_OPB_16, round_mode = ROUND_MODE_DBL
0x0002f4:          8145 0114 - NPU_SET_OFM_PRECISION          33093, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, round_mode = ROUND_MODE_NATURAL
0x0002f8:          0009 0185 - NPU_SET_IFM2_PRECISION             9, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHWC
0x0002fc:          0000 0180 - NPU_SET_IFM2_BROADCAST             0, broadcast_h = BROADCAST_MODE_DISABLE, broadcast_w = BROADCAST_MODE_DISABLE, broadcast_c = BROADCAST_MODE_DISABLE, operand_order = IFM2_OPERAND_ORDER_ORDER_B, broadcast_constant = BROADCAST_MODE_DISABLE
0x000300:          000a 018d - NPU_SET_IFM2_IB_START             10, ib_start = 10
0x000304:          0000 012f - NPU_SET_BLOCKDEP                   0, blockdep = 0
0x000308:          0000 0006 - NPU_OP_ELEMENTWISE                 0, elementwise_mode = ELEMENTWISE_MODE_MUL
// Add , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
0x00030c: 00000001 0000 4024 - NPU_SET_OFM_SCALE                  0, shift = 0, scale = 1
0x000314:          0145 0114 - NPU_SET_OFM_PRECISION            325, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16, scale_mode = OFM_SCALE_MODE_GLOBAL, round_mode = ROUND_MODE_DBL
0x000318:          0001 018f - NPU_SET_IFM2_REGION                1, region = 1
0x00031c: 00000000 0000 4080 - NPU_SET_IFM2_BASE0                 0, addr = 0x0
0x000324: 00000040 0000 4085 - NPU_SET_IFM2_STRIDE_Y              0, addr = 0x40
0x00032c: 00000040 0000 4084 - NPU_SET_IFM2_STRIDE_X              0, addr = 0x40
0x000334: 00000040 0000 4086 - NPU_SET_IFM2_STRIDE_C              0, addr = 0x40
0x00033c:          0049 0185 - NPU_SET_IFM2_PRECISION            73, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHCWB16
0x000340:          0001 0006 - NPU_OP_ELEMENTWISE                 1, elementwise_mode = ELEMENTWISE_MODE_ADD
// Mul , subOps: -, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 2, 8], IFM Block=[1, 1, 2, 8], Traversal=DepthFirst, AccType=SHRAM_Acc32
0x000344: 00000001 0032 4024 - NPU_SET_OFM_SCALE                 50, shift = 50, scale = 1
0x00034c: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000354: 00000001 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x1
0x00035c: 00000001 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x1
0x000364: 00000001 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1
0x00036c:          ff80 0118 - NPU_SET_OFM_ZERO_POINT         65408, zero_point = 65408
0x000370:          8101 0114 - NPU_SET_OFM_PRECISION          33025, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = OFM_SCALE_MODE_GLOBAL, round_mode = ROUND_MODE_NATURAL
0x000374:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x000378:          007f 0127 - NPU_SET_ACTIVATION_MAX           127, clip_boundary = 127
0x00037c:          0000 018f - NPU_SET_IFM2_REGION                0, region = 0
0x000380: 00000030 0000 4080 - NPU_SET_IFM2_BASE0                 0, addr = 0x30
0x000388: 00000004 0000 4085 - NPU_SET_IFM2_STRIDE_Y              0, addr = 0x4
0x000390: 00000004 0000 4084 - NPU_SET_IFM2_STRIDE_X              0, addr = 0x4
0x000398: 00000004 0000 4086 - NPU_SET_IFM2_STRIDE_C              0, addr = 0x4
0x0003a0:          0009 0185 - NPU_SET_IFM2_PRECISION             9, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B32, activation_format = ACTIVATION_FORMAT_NHWC
0x0003a4:          0000 0006 - NPU_OP_ELEMENTWISE                 0, elementwise_mode = ELEMENTWISE_MODE_MUL
// DMA src: Sram:FeatureMap|Staging, address: 0x0, dest: Sram:FeatureMap|Staging, address: 0x10, sizes: (N/A), length: 1
0x0003a8: 00000000 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x0
0x0003b0: 00000010 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x10
0x0003b8:          0000 0012 - NPU_OP_KERNEL_WAIT                 0, n = 0
0x0003bc:          0000 0010 - NPU_OP_DMA_START                   0
// DMA src: Sram:FeatureMap|Staging, address: 0x10, dest: Sram:FeatureMap|Staging, address: 0x0, sizes: (N/A), length: 1
0x0003c0: 00000010 0000 4030 - NPU_SET_DMA0_SRC                   0, addr = 0x10
0x0003c8: 00000000 0000 4031 - NPU_SET_DMA0_DST                   0, addr = 0x0
0x0003d0:          0000 0011 - NPU_OP_DMA_WAIT                    0, k = 0
0x0003d4:          0000 0010 - NPU_OP_DMA_START                   0
0x0003d8:          ffff 0000 - NPU_OP_STOP                    65535, mask = 65535
class GraphModule(torch.nn.Module):
    def forward(self, x, y):
        x: "f32[1, 1, 1, 1]"; y: "f32[1, 1, 1, 1]"; 
    
        x, y, = fx_pytree.tree_flatten_spec(([x, y], {}), self._in_spec)
         # File: /__w/ZephyrAI/ZephyrAI/executorch/model/aot_model.py:5 in forward, code: return x + 2.0*y
        mul: "f32[1, 1, 1, 1]" = torch.ops.aten.mul.Tensor(y, 2.0);  y = None
        add: "f32[1, 1, 1, 1]" = torch.ops.aten.add.Tensor(x, mul);  x = mul = None
        return pytree.tree_unflatten((add,), self._out_spec)
        
class GraphModule(torch.nn.Module):
    def forward(self, x, y):
        x: "f32[1, 1, 1, 1]"; y: "f32[1, 1, 1, 1]"; 
    
        x, y, = fx_pytree.tree_flatten_spec(([x, y], {}), self._in_spec)
        # No stacktrace found for following nodes
        quantize_per_tensor_default = torch.ops.quantized_decomposed.quantize_per_tensor.default(x, 0.003921568859368563, -128, -128, 127, torch.int8);  x = None
        
         # File: /__w/ZephyrAI/ZephyrAI/executorch/model/aot_model.py:5 in forward, code: return x + 2.0*y
        dequantize_per_tensor_default = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default, 0.003921568859368563, -128, -128, 127, torch.int8);  quantize_per_tensor_default = None
        
        # No stacktrace found for following nodes
        quantize_per_tensor_default_1 = torch.ops.quantized_decomposed.quantize_per_tensor.default(y, 0.003921568859368563, -128, -128, 127, torch.int8);  y = None
        
         # File: /__w/ZephyrAI/ZephyrAI/executorch/model/aot_model.py:5 in forward, code: return x + 2.0*y
        dequantize_per_tensor_default_1 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_1, 0.003921568859368563, -128, -128, 127, torch.int8);  quantize_per_tensor_default_1 = None
        quantize_per_tensor_default_2 = self._frozen_param0
        dequantize_per_tensor_default_2 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_2, 0.007843137718737125, -128, -128, 127, torch.int8);  quantize_per_tensor_default_2 = None
        mul: "f32[1, 1, 1, 1]" = torch.ops.aten.mul.Tensor(dequantize_per_tensor_default_1, dequantize_per_tensor_default_2);  dequantize_per_tensor_default_1 = dequantize_per_tensor_default_2 = None
        quantize_per_tensor_default_3 = torch.ops.quantized_decomposed.quantize_per_tensor.default(mul, 0.007843137718737125, -128, -128, 127, torch.int8);  mul = None
        dequantize_per_tensor_default_3 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_3, 0.007843137718737125, -128, -128, 127, torch.int8);  quantize_per_tensor_default_3 = None
        add: "f32[1, 1, 1, 1]" = torch.ops.aten.add.Tensor(dequantize_per_tensor_default, dequantize_per_tensor_default_3);  dequantize_per_tensor_default = dequantize_per_tensor_default_3 = None
        quantize_per_tensor_default_4 = torch.ops.quantized_decomposed.quantize_per_tensor.default(add, 0.0117647061124444, -128, -128, 127, torch.int8);  add = None
        
        # No stacktrace found for following nodes
        dequantize_per_tensor_default_4 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_4, 0.0117647061124444, -128, -128, 127, torch.int8);  quantize_per_tensor_default_4 = None
        return pytree.tree_unflatten((dequantize_per_tensor_default_4,), self._out_spec)
        
Configuration files:
   original = ['/workspace/executorch-venv/lib/python3.12/site-packages/ethosu/config_files/Arm/vela.ini']
   used = ['/workspace/executorch-venv/lib/python3.12/site-packages/ethosu/config_files/Arm/vela.ini']
System Configuration (Ethos_U55_High_End_Embedded):
   core_clock = 500000000.0
   axi0_port = Sram
   axi1_port = OffChipFlash
   Sram_clock_scales = 1.0
   Sram_burst_length = 32
   Sram_read_latency = 32
   Sram_write_latency = 32
   Dram_clock_scales = 1.0
   Dram_burst_length = 1
   Dram_read_latency = 0
   Dram_write_latency = 0
   OnChipFlash_clock_scales = 1.0
   OnChipFlash_burst_length = 1
   OnChipFlash_read_latency = 0
   OnChipFlash_write_latency = 0
   OffChipFlash_clock_scales = 0.125
   OffChipFlash_burst_length = 128
   OffChipFlash_read_latency = 64
   OffChipFlash_write_latency = 0
Memory Mode (Shared_Sram):
   const_mem_area = Axi1
   arena_mem_area = Axi0
   cache_mem_area = Axi0
   arena_cache_size = 4294967296 from Default
Architecture Settings:
   permanent_storage_mem_area = OffChipFlash
   feature_map_storage_mem_area = Sram
   fast_storage_mem_area = Sram

################################################################################
Performance for NPU Grap /tmp/tmpa9picskg/output/out
Original Operator    NNG Operator         Target Staging Usage  Peak% (Staging)  Op Cycles Network% (cycles)        NPU    SRAM AC    DRAM AC OnFlash AC OffFlash AC  MAC Count Network% (MAC)  Util% (MAC) Name                 
-------------------- -------------------- ------ ------------- ---------------- ---------- ----------------- ---------- ---------- ---------- ---------- ----------- ---------- -------------- ------------ -------------------- 
Transpose            Transpose            NPU               48            33.33         32              5.31         32          1          0          0           0          0         100.00         0.00 N/A                  
Transpose            Transpose            NPU               48            33.33         32              5.31         32          1          0          0           0          0         100.00         0.00 tosa_transpose_default_1 
Mul                  Mul                  NPU               48            33.33         32              5.31         32          2          0          0          16          0         100.00         0.00 aten_mul_tensor      
Rescale              Add                  NPU               96            66.67        256             42.45         34          2          0          0         256          0         100.00         0.00 aten_mul_tensor      
Rescale              Mul                  NPU               80            55.56         33              5.47         33          4          0          0           0          0         100.00         0.00 layer-5              
Transpose            Transpose            NPU               96            66.67         32              5.31         32          1          0          0           0          0         100.00         0.00 N/A                  
Transpose            Transpose            NPU               96            66.67         32              5.31         32          1          0          0           0          0         100.00         0.00 tosa_transpose_default 
Rescale              Add                  NPU              144           100.00         34              5.64         34          2          0          0          16          0         100.00         0.00 tosa_transpose_default 
Rescale              Mul                  NPU              128            88.89         33              5.47         33          4          0          0           0          0         100.00         0.00 layer-4              
Add                  Add                  NPU              128            88.89         21              3.48         21          4          0          0           0          0         100.00         0.00 layer-6              
Rescale              Mul                  NPU               80            55.56          2              0.33          2          2          0          0           0          0         100.00         0.00 aten_add_tensor      
Transpose            Transpose            NPU               32            22.22         32              5.31         32          1          0          0           0          0         100.00         0.00 N/A                  
Transpose            Transpose            NPU               32            22.22         32              5.31         32          1          0          0           0          0         100.00         0.00 tosa_transpose_default_2 

Network summary for out
Accelerator configuration               Ethos_U55_128
System configuration             Ethos_U55_High_End_Embedded
Memory mode                               Shared_Sram
Accelerator clock                                 500 MHz
Design peak SRAM bandwidth                       3.73 GB/s
Design peak Off-chip Flash bandwidth             0.47 GB/s

Total SRAM used                                  0.14 KiB
Total Off-chip Flash used                        0.06 KiB

CPU operators = 0 (0.0%)
NPU operators = 13 (100.0%)

Average SRAM bandwidth                           0.17 GB/s
Input   SRAM bandwidth                           0.00 MB/batch
Weight  SRAM bandwidth                           0.00 MB/batch
Output  SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth            per input      0.00 MB/inference (batch size 1)

Average Off-chip Flash bandwidth                 0.23 GB/s
Input   Off-chip Flash bandwidth                 0.00 MB/batch
Weight  Off-chip Flash bandwidth                 0.00 MB/batch
Output  Off-chip Flash bandwidth                 0.00 MB/batch
Total   Off-chip Flash bandwidth                 0.00 MB/batch
Total   Off-chip Flash bandwidth  per input      0.00 MB/inference (batch size 1)

Original Weights Size                            0.00 KiB
NPU Encoded Weights Size                         0.00 KiB

Neural network macs                                 0 MACs/batch

Info: The numbers below are internal compiler estimates.
For performance numbers the compiled network should be run on an FVP Model or FPGA.

Network Tops/s                                   0.00 Tops/s

NPU cycles                                        381 cycles/batch
SRAM Access cycles                                 26 cycles/batch
DRAM Access cycles                                  0 cycles/batch
On-chip Flash Access cycles                         0 cycles/batch
Off-chip Flash Access cycles                      288 cycles/batch
Total cycles                                      603 cycles/batch

Batch Inference time                 0.00 ms, 829187.40 inferences/s (batch size 1)

class GraphModule(torch.nn.Module):
    def forward(self, x, y):
        x: "f32[1, 1, 1, 1]"; y: "f32[1, 1, 1, 1]"; 
    
        x, y, = fx_pytree.tree_flatten_spec(([x, y], {}), self._in_spec)
        # No stacktrace found for following nodes
        alloc: "i8[1, 1, 1, 1]" = executorch_exir_memory_alloc(((1, 1, 1, 1), torch.int8))
        quantized_decomposed_quantize_per_tensor_default: "i8[1, 1, 1, 1]" = torch.ops.quantized_decomposed.quantize_per_tensor.out(x, 0.003921568859368563, -128, -128, 127, torch.int8, out = alloc);  x = alloc = None
        alloc_1: "i8[1, 1, 1, 1]" = executorch_exir_memory_alloc(((1, 1, 1, 1), torch.int8))
        quantized_decomposed_quantize_per_tensor_default_1: "i8[1, 1, 1, 1]" = torch.ops.quantized_decomposed.quantize_per_tensor.out(y, 0.003921568859368563, -128, -128, 127, torch.int8, out = alloc_1);  y = alloc_1 = None
        lowered_module_0 = self.lowered_module_0
        executorch_call_delegate = torch.ops.higher_order.executorch_call_delegate(lowered_module_0, quantized_decomposed_quantize_per_tensor_default, quantized_decomposed_quantize_per_tensor_default_1);  lowered_module_0 = quantized_decomposed_quantize_per_tensor_default = quantized_decomposed_quantize_per_tensor_default_1 = None
        getitem: "i8[1, 1, 1, 1]" = executorch_call_delegate[0];  executorch_call_delegate = None
        alloc_2: "f32[1, 1, 1, 1]" = executorch_exir_memory_alloc(((1, 1, 1, 1), torch.float32))
        quantized_decomposed_dequantize_per_tensor_default: "f32[1, 1, 1, 1]" = torch.ops.quantized_decomposed.dequantize_per_tensor.out(getitem, 0.0117647061124444, -128, -128, 127, torch.int8, out = alloc_2);  getitem = alloc_2 = None
        return pytree.tree_unflatten((quantized_decomposed_dequantize_per_tensor_default,), self._out_spec)
        
