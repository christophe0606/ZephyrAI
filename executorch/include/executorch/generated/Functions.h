/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
#pragma once

#include <tuple>

#include <executorch/runtime/core/exec_aten/exec_aten.h> // at::Tensor etc.
#include <executorch/codegen/macros.h> // TORCH_API
#include <executorch/runtime/kernel/kernel_runtime_context.h>

// @generated by gen.py from Functions.h

#include "NativeFunctions.h"

namespace torch {
namespace executor {


namespace quantized_decomposed {

// quantized_decomposed::dequantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, ScalarType? out_dtype=None, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & dequantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::dequantize_per_tensor_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out_dtype, out);
}


// quantized_decomposed::quantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out) {
    return ::torch::executor::native::quantize_per_tensor_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out);
}

} // namespace quantized_decomposed

} // namespace executor
} // namespace torch
